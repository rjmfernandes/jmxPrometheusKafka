2\. Which metrics give them what they want They asked for three things. Here’s how to get each, and from where. 2.1 Message volume in number and bytes Broker‑side (per topic / per cluster): Use broker JMX MBeans kafka.server:type=BrokerTopicMetrics: MessagesInPerSec → total and rate of messages per topic BytesInPerSec, BytesOutPerSec → total and rate of bytes per topic With the JMX Exporter, these become Prometheus metrics like (examples): kafka\_server\_brokertopicmetrics\_messagesin\_total{topic="..."} kafka\_server\_brokertopicmetrics\_bytesin\_total{topic="..."} kafka\_server\_brokertopicmetrics\_bytesout\_total{topic="..."} plus the corresponding \_rate series if you use the default rules from our monitoring stacks. Consumer‑side (how much consumers actually read): Use client JMX metrics from kafka.consumer:type=consumer-fetch-manager-metrics,client-id=...: records-consumed-total, records-consumed-rate bytes-consumed-rate With the client JMX Exporter config, these show up as metrics such as: kafka\_consumer\_consumer\_fetch\_manager\_metrics\_records\_consumed\_total{client\_id="..."} kafka\_consumer\_consumer\_fetch\_manager\_metrics\_bytes\_consumed\_rate{client\_id="..."} 2.2 Average consumption per second of messages and bytes Once JMX Exporter is in place, Prometheus can compute this directly from the client metrics: Messages per second (per consumer group or app): sum by (group, client\_id) ( rate(kafka\_consumer\_consumer\_fetch\_manager\_metrics\_records\_consumed\_total\[5m\]) ) Bytes per second: sum by (group, client\_id) ( rate(kafka\_consumer\_consumer\_fetch\_manager\_metrics\_bytes\_consumed\_total\[5m\]) ) (naming may vary slightly depending on their exact rules file, but this is the pattern you get from the Confluent monitoring stacks.) 2.3 Waiting time from production to commit (end‑to‑end delay) Kafka/JMX does not expose a direct “production timestamp → commit timestamp” metric. You have three realistic options: Approximate latency from lag in records Use records-lag-max / records-lag-avg from consumer-fetch-manager-metrics. Combine with producer message rate (from MessagesInPerSec) to estimate seconds of lag: # VERY rough approximation, cluster‑wide sum(max\_over\_time(kafka\_consumer\_consumer\_fetch\_manager\_metrics\_records\_lag\_max\[5m\])) / sum(rate(kafka\_server\_brokertopicmetrics\_messagesin\_total\[5m\])) This assumes relatively stable production and is best used as an order‑of‑magnitude indicator, not a hard SLA figure. Use consumer‑lag‑specific tooling For self‑managed, there’s kafka-lag-exporter that derives lag in seconds and exports it to Prometheus (not Confluent‑supported, but commonly used and mentioned in our internal enablement). Best (but app‑level) option: instrument the application Include a production timestamp in the record value or headers; on the consumer, compute now - recordTimestamp at commit time and export that as a custom Prometheus metric. This gives a true end‑to‑end latency number, which JMX alone cannot provide. For an SLA‑style “difference between production and commit time”, I’d explicitly tell them that JMX alone can’t give an exact value; they either accept an approximation from lag, or add app‑level metrics. 3. How to export these metrics with JMX Exporter → Prometheus The pattern is the same in Spain as anywhere else: run jmx\_prometheus\_javaagent alongside Kafka (and optionally the Java consumers) and let Prometheus scrape /metrics. 3.1 Attach JMX Exporter to brokers / clients From our internal KB on this topic: Download jmx\_prometheus\_javaagent-.jar (we currently recommend a 1.x version; the Confluent dashboards are built for 1.x). Place a config file, e.g.: kafka\_broker.yml for brokers kafka\_client.yml for producers/consumers You can start from the reference configs in the Confluent jmx‑monitoring‑stacks repo: https://github.com/confluentinc/jmx-monitoring-stacks/tree/main/jmxexporter-prometheus-grafana For a broker, set the KAFKA\_OPTS environment variable (or the equivalent per component) to include the Java agent, for example: export KAFKA\_OPTS="$KAFKA\_OPTS -javaagent:/opt/prometheus/jmx\_prometheus\_javaagent-1.1.0.jar=1234:/opt/prometheus/kafka\_broker.yml" Then restart the broker so the wrapper scripts (kafka-run-class etc.) pick it up. Verify locally: curl http://:1234/metrics You should see metrics like kafka\_server\_brokertopicmetrics\_messagesin\_total etc. The same pattern applies to Java consumers/producers: add the -javaagent line to their JVM options, pointing at a client‑focused rules file (see kafka\_clients\*.yml in the monitoring stacks repo). Important: especially for large clusters with many topics/partitions, JMX Exporter can become slow if you expose everything. The recommended practice (and what jmx‑monitoring‑stacks does) is to whitelist only the MBeans you care about to reduce load and avoid timeouts. 3.2 Configure Prometheus to scrape JMX Exporter Following our blog and KB: In prometheus.yml, add a scrape\_config entry like: scrape\_configs: - job\_name: 'kafka-brokers' static\_configs: - targets: - 'broker1:1234' - 'broker2:1234' labels: env: 'prod' Reload/restart Prometheus and confirm the new targets are UP in the Prometheus UI. 4. References you can share with the customer If you want to reply with links: Kafka / Confluent Platform JMX metrics overview https://docs.confluent.io/platform/current/kafka/monitoring.html Confluent monitoring stacks for JMX Exporter + Prometheus + Grafana https://github.com/confluentinc/jmx-monitoring-stacks/tree/main/jmxexporter-prometheus-grafana Internal (Confluent) KB – how to attach JMX Exporter to CP components and clients “How to expose JMX metrics to Prometheus using JmxExporter” Internal KB – troubleshooting JMX Exporter overload and whitelisting metrics “How to troubleshoot prometheus JMX exporter issues” You can safely tell the customer that: There was no special Spain‑specific recipe shared in the 13/01 OH. The standard supported pattern is: CP JMX → JMX Exporter javaagent → Prometheus → Grafana, using the Confluent jmx‑monitoring‑stacks configs as a starting point, plus app‑level metrics if they need precise end‑to‑end latency. Would you like a ready-to-use JMX Exporter config for your brokers/clients or help tailoring it to your setup?